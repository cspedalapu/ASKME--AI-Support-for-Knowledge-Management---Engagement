# ASKME â€“ AI Support for Knowledge Management & Engagement

**ASKME** is an intelligent GenAI-powered assistant designed to transform university support services. It delivers real-time, context-aware, and multilingual responses to student queries â€” with a special focus on international student needs â€” through advanced NLP, Retrieval-Augmented Generation (RAG), and fine-tuned Large Language Models (LLMs).

---

## Problem Statement

University departments are overwhelmed by thousands of repetitive queries each semester, resulting in:

- Long response delays during peak periods
- Inconsistent or incomplete information from different departments
- High dependency on limited staff hours (8 AM â€“ 5 PM)
- Lack of 24/7, personalized assistance for critical student issues (visa, scholarship, course selection, etc.)

**CampusNavigator-AI** addresses these issues by providing an AI-driven, always-available support system tailored to the academic environment.

---

## Objectives

- âœ… Automate academic and administrative FAQ responses using LLMs
- âœ… Support personalized and multilingual responses with context retention
- âœ… Reduce human staff workload and response wait times
- âœ… Build a scalable and accurate AI assistant for university environments
- âœ… Enable document-level understanding from PDF, web, and JSON content
- âœ… Track and log queries for analytics, feedback, and improvement

---

## Tech Stack

| Component             | Tools / Frameworks                                      |
|----------------------|----------------------------------------------------------|
| **LLMs**             | BERT, T5, Sentence-BERT, LLaMA (Phase-2)                |
| **NLP Frameworks**   | HuggingFace Transformers, LangChain                     |
| **Embeddings**       | Word2Vec, TF-IDF, SentenceTransformers                  |
| **RAG Framework**    | FAISS / ChromaDB + LLM-based Generator                  |
| **Data Collection**  | BeautifulSoup, Selenium, OCR (Tesseract)                |
| **Preprocessing**    | spaCy, NLTK, Regex-based cleaning                       |
| **Backend/API**      | FastAPI                                                  |
| **Frontend (UI)**    | Streamlit (initial), React (optional future phase)      |
| **Deployment**       | AWS EC2, S3, Lambda (Future), GitHub Actions (CI/CD)    |
| **Datasets**         | UNT scraped content (.pdf/.json), SQuAD, internal logs  |

---

## Project Modules

1. **Data Scraping & Cleaning**
   - Web scraping with BeautifulSoup/Selenium
   - OCR from scanned PDFs using Tesseract
   - Store data in structured JSON/PDF formats

2. **Preprocessing Pipeline**
   - Tokenization, Lemmatization, Stopword removal
   - POS tagging, Named Entity Recognition (NER)
   - Extractive and abstractive summarization

3. **Knowledge Base Builder**
   - Index structured documents using FAISS or ChromaDB
   - Embed documents using Sentence-BERT for similarity search

4. **RAG Pipeline**
   - Combine retrieval with LLM (T5/BERT)
   - Contextual matching + dynamic answer generation

5. **Evaluation**
   - Metrics: F1 Score, Exact Match, Precision, Recall
   - User satisfaction surveys and feedback loop
   - Accuracy benchmarking using external datasets

6. **Deployment**
   - Backend with FastAPI and REST endpoints
   - Initial UI with Streamlit; optional migration to React
   - Hosting on AWS with future Lambda integration

---
CampusNavigator-AI/
â”œâ”€â”€ README.md                           â† Project documentation overview
â”œâ”€â”€ requirements.txt                    â† Python dependencies
â”œâ”€â”€ .env.example                        â† Example environment variables
â”œâ”€â”€ .gitignore                          â† Files to ignore
â”‚
â”œâ”€â”€ backend/                            â† Core AI/NLP backend logic
â”‚   â”œâ”€â”€ api/                            â† FastAPI app with routes
â”‚   â”‚   â”œâ”€â”€ main.py                     â† FastAPI entrypoint
â”‚   â”‚   â”œâ”€â”€ routes.py                   â† Endpoint definitions
â”‚   â”‚   â””â”€â”€ models.py                   â† Pydantic models for request/response
â”‚   â”‚
â”‚   â”œâ”€â”€ rag_engine/                     â† Retrieval-Augmented Generation
â”‚   â”‚   â”œâ”€â”€ retriever.py                â† Vector search logic (FAISS/Chroma)
â”‚   â”‚   â”œâ”€â”€ generator.py                â† LLM response generation (T5/BERT)
â”‚   â”‚   â””â”€â”€ pipeline.py                 â† End-to-end RAG pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/                     â† Word/sentence embedding logic
â”‚   â”‚   â”œâ”€â”€ embed_utils.py              â† Sentence-BERT, TF-IDF, Word2Vec loaders
â”‚   â”‚   â””â”€â”€ faiss_index/                â† Saved vector indices
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/                  â† Data cleaning & text preparation
â”‚   â”‚   â””â”€â”€ cleaner.py                  â† Tokenization, Lemmatization, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                           â† Local dataset handling
â”‚   â”‚   â”œâ”€â”€ raw/                        â† Scraped PDFs, HTMLs
â”‚   â”‚   â””â”€â”€ processed/                  â† Cleaned structured data
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                         â† Fine-tuned and custom-trained models
â”‚   â”‚   â”œâ”€â”€ bert_tuned/
â”‚   â”‚   â””â”€â”€ t5_adapter/
â”‚   â”‚
â”‚   â””â”€â”€ notebooks/                      â† Research & prototyping (Colab ready)
â”‚       â”œâ”€â”€ 1_scraping_colab.ipynb
â”‚       â”œâ”€â”€ 2_preprocessing.ipynb
â”‚       â”œâ”€â”€ 3_embeddings.ipynb
â”‚       â”œâ”€â”€ 4_rag_pipeline.ipynb
â”‚       â”œâ”€â”€ 5_evaluation.ipynb
â”‚       â””â”€â”€ 6_interface_streamlit.ipynb
â”‚
â”œâ”€â”€ frontend/                           â† UI layer (Streamlit, React later)
â”‚   â”œâ”€â”€ streamlit_app/                  â† Initial chatbot with Streamlit
â”‚   â”‚   â”œâ”€â”€ app.py                      â† Streamlit UI script
â”‚   â”‚   â””â”€â”€ components/                 â† Optional reusable UI blocks
â”‚   â””â”€â”€ react_app/                      â† (Optional) Production-grade React UI
â”‚       â”œâ”€â”€ public/
â”‚       â”œâ”€â”€ src/
â”‚       â””â”€â”€ package.json
â”‚
â”œâ”€â”€ tests/                              â† Unit + integration tests
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_preprocessing.py
â”‚
â”œâ”€â”€ scripts/                            â† âš™ï¸ Deployment / automation scripts
â”‚   â”œâ”€â”€ deploy_aws.sh                   â† AWS deployment setup
â”‚   â””â”€â”€ ingest_data.py                  â† CLI script to scrape and prepare data
â”‚
â”œâ”€â”€ .dockerignore                       â† ğŸ³ Docker ignore
â”œâ”€â”€ Dockerfile                          â† ğŸ³ Backend containerization
â”œâ”€â”€ docker-compose.yml                  â† ğŸ³ Local full-stack deployment
â””â”€â”€ LICENSE                             â† ğŸ“„ License
